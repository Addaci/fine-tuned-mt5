# Install necessary libraries
!pip install --upgrade transformers datasets

# Mount Google Drive to Colab
from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer
from datasets import Dataset, DatasetDict

# Define file paths
csv_path = "/content/drive/MyDrive/Colab Notebooks/HCA_1358_Paired_Line_Text_Ver.1.2_15102024.csv"

# Load the CSV data into a Pandas DataFrame
df = pd.read_csv(csv_path)  # Pandas automatically detects the header

# Get the raw_htr and groundtruth lists from the DataFrame
raw_htr_texts = df['Raw-HTR Text'].tolist()  # Accessing by column name
groundtruth_texts = df['Hand-corrected Groundtruth'].tolist()

# Initialize the T5 tokenizer
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

# Tokenize the data
def preprocess_function(examples):
    inputs = [doc for doc in examples['Raw-HTR Text']]  # Using column name
    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")

    with tokenizer.as_target_tokenizer():
        # Corrected line: Ensuring the string is properly terminated
        labels = tokenizer(examples['Hand-corrected Groundtruth'], max_length=128, truncation=True, padding="max_length")  

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Create a Hugging Face Dataset
data = Dataset.from_dict({"Raw-HTR Text": raw_htr_texts, "Hand-corrected Groundtruth": groundtruth_texts})  # Using column names
tokenized_data = data.map(preprocess_function, batched=True)

# Split the data into training and validation sets
train_testvalid = tokenized_data.train_test_split(test_size=0.2)
test_valid = train_testvalid['test'].train_test_split(test_size=0.5)
dataset = DatasetDict({
    'train': train_testvalid['train'],
    'test': test_valid['test'],  
  
    'validation': test_valid['train']})  
  

# Initialize the mT5-small model
model = T5ForConditionalGeneration.from_pretrained("google/mt5-small")

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,  # Adjust based on your GPU memory
    per_device_eval_batch_size=8,
    # predict_with_generate=True,  # Removed in previous step
    learning_rate=2e-5,  # You can experiment with different learning rates
    num_train_epochs=3,  # Start with a few epochs and adjust
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    # Early stopping parameters:
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    # early_stopping_patience=3, # Removed for now
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"]
)

# Fine-tune the model
trainer.train()  
  

# Evaluate the model
eval_results = trainer.evaluate()
print(f"Evaluation results: {eval_results}")
